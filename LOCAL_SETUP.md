# ğŸš€ LOCAL DEVELOPMENT SETUP GUIDE

This guide helps you run the Databricks ETL Pipeline locally on your machine for development and testing.

## Prerequisites

### Java Installation (Required for Spark)
1. **Download Java 8 or 11:**
   - Oracle JDK: https://www.oracle.com/java/technologies/downloads/
   - OpenJDK: https://adoptium.net/
   
2. **Set JAVA_HOME environment variable:**
   ```powershell
   # In PowerShell (as Administrator)
   [Environment]::SetEnvironmentVariable("JAVA_HOME", "C:\Program Files\Java\jdk-11.0.x", "Machine")
   ```

3. **Verify Java installation:**
   ```cmd
   java -version
   ```

### Python Dependencies
Run the batch script to install all required packages:
```cmd
install_local_deps.bat
```

Or install manually:
```cmd
pip install -r requirements.txt
```

## Quick Start

### 1. Run Complete Pipeline
Execute the full ETL pipeline locally:
```cmd
python run_local_pipeline.py
```

This will:
- âœ… Generate sample movie data
- âœ… Process Bronze â†’ Silver â†’ Gold layers
- âœ… Create business analytics
- âœ… Generate visualizations
- âœ… Store everything in local Delta tables

### 2. Run Individual Components

**Generate Sample Data:**
```cmd
python local_demo_data.py
```

**Test Spark Configuration:**
```python
from config.local_config import local_config
spark = local_config.get_spark_session()
spark.sql("SELECT 'Hello Local Spark!' as message").show()
```

## Project Structure (Local Mode)

```
ğŸ“¦ Project Root
â”œâ”€â”€ ğŸ“ data/                    # Local Delta Lake storage
â”‚   â”œâ”€â”€ ğŸ“ bronze/             # Raw ingested data
â”‚   â”œâ”€â”€ ğŸ“ silver/             # Cleaned & validated data  
â”‚   â”œâ”€â”€ ğŸ“ gold/               # Business analytics tables
â”‚   â”œâ”€â”€ ğŸ“ checkpoints/        # Spark streaming checkpoints
â”‚   â””â”€â”€ ğŸ“ raw/                # Sample input data
â”œâ”€â”€ ğŸ“ local_visualizations/   # Generated charts & graphs
â”œâ”€â”€ ğŸ“ config/                 # Configuration files
â”œâ”€â”€ ğŸ run_local_pipeline.py   # Main execution script
â”œâ”€â”€ ğŸ local_demo_data.py      # Sample data generator
â””â”€â”€ ğŸ“„ requirements.txt        # Python dependencies
```

## Data Flow (Local)

```mermaid
graph LR
    A[Sample Data] --> B[Bronze Layer]
    B --> C[Silver Layer] 
    C --> D[Gold Layer]
    D --> E[Visualizations]
    D --> F[Business Analytics]
```

## Expected Output

When you run `python run_local_pipeline.py`, you'll see:

```
ğŸš€ DATABRICKS ETL PIPELINE - LOCAL EXECUTION
============================================================
â° Started at: 2025-09-29 10:30:00

ğŸ”§ Setting up local environment...
âœ… Configuration loaded successfully

ğŸ“¦ Preparing sample data...
ğŸ¬ Generating sample movie data...
âœ… Sample data created:
   ğŸ“ JSON: data/raw/movies.json
   ğŸ“ CSV: data/raw/movies.csv  
   ğŸ“Š Records: 20

âš¡ Initializing Spark session...
âœ… Spark session created successfully

ğŸ¥‰ BRONZE LAYER - Raw Data Ingestion
==================================================
ğŸ“Š Loaded 20 records from sample data
âœ… Bronze data written to: data/bronze/movies

ğŸ¥ˆ SILVER LAYER - Data Cleaning & Validation  
==================================================
âœ… Silver data written to: data/silver/movies_clean
ğŸ“Š Cleaned records: 20

ğŸ¥‡ GOLD LAYER - Business Analytics
==================================================
âœ… Gold tables created:
   ğŸ“ Genre Analysis: data/gold/genre_analysis
   ğŸ“ Era Analysis: data/gold/era_analysis
   ğŸ“ Top Movies: data/gold/top_movies

ğŸ“Š BUSINESS INSIGHTS:
ğŸ­ Genre Performance:
[Shows revenue by genre]

ğŸ•°ï¸ Era Analysis:  
[Shows trends by era]

ğŸ† Top 5 Movies by Revenue:
[Shows top performers]

ğŸ“ˆ CREATING VISUALIZATIONS
==================================================
âœ… Visualizations saved to: local_visualizations/
   ğŸ“Š genre_revenue.png
   ğŸ“Š era_ratings.png

ğŸ‰ PIPELINE COMPLETED SUCCESSFULLY!
============================================================
â±ï¸  Total execution time: 12.34 seconds
ğŸ“ Data stored in: data/
ğŸ“Š Layers processed: Bronze â†’ Silver â†’ Gold
âœ¨ Ready for analysis and visualization!
```

## Troubleshooting

### Common Issues

**1. Java Not Found:**
```
ERROR: JAVA_HOME is not set
```
**Solution:** Install Java and set JAVA_HOME environment variable

**2. Memory Issues:**
```
java.lang.OutOfMemoryError
```  
**Solution:** Increase Spark driver memory in `local_config.py`:
```python
.config("spark.driver.memory", "4g")
.config("spark.driver.maxResultSize", "2g")
```

**3. Permission Errors:**
```
PermissionError: [WinError 5] Access is denied
```
**Solution:** Run command prompt as Administrator

**4. Delta Lake Issues:**
```
ClassNotFoundException: delta.sql.DeltaSparkSessionExtension
```
**Solution:** Ensure delta-spark is properly installed:
```cmd
pip install delta-spark==3.0.0
```

### Checking Installation

**Verify Spark:**
```python
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("test").getOrCreate()
print("Spark version:", spark.version)
```

**Verify Delta Lake:**  
```python
from delta import DeltaTable
print("Delta Lake imported successfully")
```

## Next Steps

After running locally:

1. **Explore the Data:**
   - Check `data/` folder for Delta tables
   - View `local_visualizations/` for charts

2. **Customize the Pipeline:**
   - Modify `local_demo_data.py` for different datasets
   - Update transformations in the pipeline script

3. **Deploy to Azure:**
   - Use the Azure notebooks for cloud deployment
   - Configure Azure resources using ARM templates

4. **Integrate with BI Tools:**
   - Export data using `notebooks/05_Data_Export_Hub.py`
   - Connect Power BI, Tableau, or Excel

## Performance Tips

- **Reduce Logging:** Set `spark.sparkContext.setLogLevel("ERROR")` for cleaner output
- **Partition Data:** For larger datasets, partition by date or category
- **Cache DataFrames:** Use `.cache()` for DataFrames used multiple times
- **Optimize Spark:** Adjust memory settings based on your machine specs

---

**Happy Local Development! ğŸš€**